package fr.mosef.scala.template

import fr.mosef.scala.template.job.Job
import fr.mosef.scala.template.processor.Processor
import fr.mosef.scala.template.processor.impl.ProcessorImpl
import fr.mosef.scala.template.reader.Reader
import fr.mosef.scala.template.reader.impl.{CSVReader, HiveReader, ParquetReader}
import fr.mosef.scala.template.writer.Writer
import fr.mosef.scala.template.writer.impl.{CSVOutputWriter, ParquetOutputWriter, HiveTableWriter}

import org.apache.spark.sql.SparkSession

object Main extends App with Job {

  // RÃ©cupÃ©ration des arguments CLI avec des valeurs par dÃ©faut
  val argsList = args

  val masterUrl: String = if (argsList.length > 0) argsList(0) else "local[1]"

  val inputPath: String = if (argsList.length > 1) argsList(1) else {
    println("âš ï¸  Aucun chemin dâ€™entrÃ©e fourni.")
    sys.exit(1)
  }

  val outputPath: String = if (argsList.length > 2) argsList(2) else "./default/output-writer"
  val groupColumn: String = if (argsList.length > 3) argsList(3) else "group_key"
  val targetColumn: String = if (argsList.length > 4) argsList(4) else "field1"
  val useHive: Boolean = if (argsList.length > 5) argsList(5).toBoolean else false
  val propertiesPath: String = if (argsList.length > 6) argsList(6) else "./src/main/resources/application.properties"

  println("=" * 100)
  println("ðŸ”§ ParamÃ¨tres de l'application")
  println("=" * 100)
  println(s"ðŸ”— Spark master : $masterUrl")
  println(s"ðŸ“¥ Chemin source : $inputPath")
  println(s"ðŸ“¤ Chemin destination : $outputPath")
  println(s"ðŸ”‘ Colonne de regroupement : $groupColumn")
  println(s"ðŸ“Š Colonne d'opÃ©ration : $targetColumn")
  println(s"ðŸ“ Fichier de configuration : $propertiesPath")
  println(s"ðŸ Utilisation de Hive : $useHive")
  println("=" * 100)

  // Initialisation de Spark
  val spark = SparkSession
    .builder()
    .master(masterUrl)
    .enableHiveSupport()
    .getOrCreate()

  spark.sparkContext.setLogLevel("ERROR")

  // DÃ©termination du type de fichier (csv, parquet, etc.)
  val fileType = inputPath.split("\\.").lastOption.getOrElse("")

  // SÃ©lection du lecteur en fonction du format et du mode Hive
  val reader: Reader = if (useHive) {
    new HiveReader(spark, propertiesPath, fileType)
  } else {
    fileType match {
      case "csv" => new CSVReader(spark, propertiesPath)
      case "parquet" => new ParquetReader(spark, propertiesPath)
      case _ =>
        println(s"âŒ Format de fichier non pris en charge : $fileType")
        sys.exit(1)
    }
  }
  // CrÃ©ation du processeur
  val processor: Processor = new ProcessorImpl(groupColumn, targetColumn)

  // Lecture des donnÃ©es
  val inputDF = reader.read(inputPath)

  println("\nðŸ§¾ AperÃ§u des donnÃ©es d'entrÃ©e")
  inputDF.show(5)

  // ExÃ©cution des traitements
  val groupedDF = processor.groupby(inputDF)
  val summedDF = processor.computeSum(inputDF)
  val stddevDF = processor.computeStdDev(inputDF)

  // Ã‰criture des rÃ©sultats (fichiers ou Hive selon le mode)
  if (!useHive) {
    val dst_path = outputPath

    val writer: Writer = fileType match {
      case "csv" => new CSVOutputWriter(propertiesPath)
      case "parquet" => new ParquetOutputWriter()
      case _ => sys.exit(1)
    }

    println("\nðŸ“ˆ RÃ©sultat - GroupBy")
    groupedDF.show(5)

    println("\nâž• RÃ©sultat - Somme")
    summedDF.show(5)

    println("\nðŸ“‰ RÃ©sultat - Ã‰cart-type")
    stddevDF.show(5)

    writer.write(groupedDF, "overwrite", dst_path + "_groupby")
    writer.write(summedDF, "overwrite", dst_path + "_sum")
    writer.write(stddevDF, "overwrite", dst_path + "_stddev")

  } else {
    // Cas Hive
    val writer = new HiveTableWriter(spark)
    writer.writeToTable(groupedDF, "table_groupby")
    writer.writeToTable(summedDF, "table_sum")
    writer.writeToTable(stddevDF, "table_stddev")
  }
}
